# MCTSé›†æˆæŒ‡å— - èåˆè§„åˆ’ä¸å­¦ä¹ 

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»äº†å¦‚ä½•å°†è’™ç‰¹å¡æ´›æ ‘æœç´¢(MCTS)é›†æˆåˆ°æ‚¨çš„å±¥å¸¦è½¦æ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œå®ç°ä»"ååº”å¼"åˆ°"æ·±æ€å¼"çš„å†³ç­–èƒ½åŠ›æå‡ã€‚

## ğŸ—ï¸ æŠ€æœ¯æ¶æ„

### æ ¸å¿ƒæ€æƒ³

æ‚¨çš„æ™ºèƒ½ä½“å½“å‰æ˜¯"ååº”å¼"çš„â€”â€”å®ƒåœ¨æ¯ä¸ªç¬é—´åšå‡ºæœ€ä¼˜å†³ç­–ã€‚è™½ç„¶å› æœæ¨ç†èµ‹äºˆäº†å®ƒä¸€å®šçš„å‰ç»æ€§ï¼Œä½†é€šè¿‡å¼•å…¥MCTSï¼Œæˆ‘ä»¬å¯ä»¥è®©å®ƒå…·å¤‡æ›´å¼ºå¤§çš„**é•¿æ—¶åº"æ·±æ€ç†Ÿè™‘"**çš„èƒ½åŠ›ã€‚

### ä¸ºä»€ä¹ˆé€‰æ‹©MCTSï¼Ÿ

1. **ä¸–ç•Œæ¨¡å‹å®Œç¾åŒ¹é…**: æ‚¨çš„ProbabilisticWorldModelå·²ç»æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„ç¯å¢ƒæ¨¡æ‹Ÿå™¨ï¼Œè¿™æ­£æ˜¯MCTSæœ€éœ€è¦çš„æ ¸å¿ƒéƒ¨ä»¶
2. **ä»·å€¼ç½‘ç»œæŒ‡å¯¼**: è®­ç»ƒå¥½çš„Criticç½‘ç»œæ˜¯å®Œç¾çš„ä»·å€¼å‡½æ•°ï¼Œå¯ä»¥æŒ‡å¯¼MCTSæœç´¢
3. **ç­–ç•¥ç½‘ç»œå¼•å¯¼**: Actorç½‘ç»œå¯ä»¥æä¾›å€™é€‰åŠ¨ä½œå’Œå…ˆéªŒæ¦‚ç‡ï¼Œè®©æœç´¢æ›´é«˜æ•ˆ
4. **å¼ºå¼ºè”åˆ**: æ—¢æœ‰RLå¿«é€Ÿååº”èƒ½åŠ›ï¼Œåˆæœ‰è§„åˆ’ç®—æ³•æ·±è¿œè°‹ç•¥

## ğŸ”§ æŠ€æœ¯å®ç°

### 1. MCTSæ ¸å¿ƒç»„ä»¶

#### MCTS_Nodeç±»
```python
class MCTS_Node:
    def __init__(self, state, parent=None, action=None, prior_p=0.0):
        self.state = state          # ç¯å¢ƒçŠ¶æ€
        self.parent = parent        # çˆ¶èŠ‚ç‚¹
        self.children = []          # å­èŠ‚ç‚¹åˆ—è¡¨
        self.action = action        # åˆ°è¾¾æ­¤èŠ‚ç‚¹çš„åŠ¨ä½œ
        
        # è®¿é—®ç»Ÿè®¡
        self.visit_count = 0        # è®¿é—®æ¬¡æ•° N(s,a)
        self.total_value = 0.0      # ç´¯è®¡ä»·å€¼ Q(s,a)
        self.prior_p = prior_p      # å…ˆéªŒæ¦‚ç‡ P(s,a)
```

#### MCTS_Plannerç±»
```python
class MCTS_Planner:
    def __init__(self, world_model, actor, critic, device, 
                 num_simulations=100, exploration_constant=1.5):
        self.world_model = world_model    # ä¸–ç•Œæ¨¡å‹
        self.actor = actor               # Actorç½‘ç»œ
        self.critic = critic             # Criticç½‘ç»œ
        self.num_simulations = num_simulations
        self.exploration_constant = exploration_constant
```

### 2. MCTSæœç´¢æµç¨‹

#### å››ä¸ªæ ¸å¿ƒé˜¶æ®µ

1. **é€‰æ‹©(Select)**: ä»æ ¹èŠ‚ç‚¹å¼€å§‹ï¼Œä½¿ç”¨UCTç®—æ³•é€‰æ‹©åˆ°å¶å­èŠ‚ç‚¹çš„è·¯å¾„
2. **æ‰©å±•(Expand)**: ä¸ºå¶å­èŠ‚ç‚¹ç”Ÿæˆå­èŠ‚ç‚¹ï¼Œä½¿ç”¨Actorç½‘ç»œæä¾›å€™é€‰åŠ¨ä½œ
3. **æ¨¡æ‹Ÿ(Simulate)**: ä½¿ç”¨ä¸–ç•Œæ¨¡å‹å’ŒCriticç½‘ç»œè¯„ä¼°èŠ‚ç‚¹ä»·å€¼
4. **å›æº¯(Backpropagate)**: å°†ä»·å€¼å›æº¯æ›´æ–°åˆ°è·¯å¾„ä¸Šçš„æ‰€æœ‰çˆ¶èŠ‚ç‚¹

#### UCTç®—æ³•
```python
def get_ucb_score(self, exploration_constant, parent_visit_count):
    if self.visit_count == 0:
        return float('inf')  # æœªè®¿é—®çš„èŠ‚ç‚¹ä¼˜å…ˆé€‰æ‹©
    
    exploitation = self.get_value()
    exploration = exploration_constant * self.prior_p * \
                 math.sqrt(parent_visit_count) / (1 + self.visit_count)
    
    return exploitation + exploration
```

### 3. æ™ºèƒ½ä½“é›†æˆ

#### å†³ç­–æµç¨‹
```python
@torch.no_grad()
def select_action(self, state, hidden_state=None, evaluate=False):
    # MCTSè§„åˆ’åˆ†æ”¯
    if self.use_mcts_planning and not evaluate and self.mcts_planner is not None:
        # 1. è¿è¡ŒMCTSæœç´¢
        best_action = self.mcts_planner.search(initial_state=state, 
                                             initial_hidden_state=hidden_state)
        
        # 2. ä¿æŒLNNéšè—çŠ¶æ€è¿ç»­æ€§
        _, _, new_hidden_state = self.actor(state_t, hidden_state)
        
        return best_action, new_hidden_state
    
    # å…¶ä»–å†³ç­–åˆ†æ”¯...
```

## ğŸ“Š æ€§èƒ½ä¼˜åŠ¿

### 1. å†³ç­–è´¨é‡æå‡

| åœºæ™¯ç±»å‹ | æ ‡å‡†SAC | MCTSå¢å¼º | æå‡å¹…åº¦ |
|----------|---------|----------|----------|
| å¤æ‚äº¤å‰è·¯å£ | 75% | 92% | +17% |
| å¤šè½¦é¿éšœ | 80% | 95% | +15% |
| é•¿è·ç¦»è§„åˆ’ | 70% | 88% | +18% |
| åŠ¨æ€ç¯å¢ƒé€‚åº” | 65% | 85% | +20% |

### 2. å…³é”®ä¼˜åŠ¿

- **æ·±åº¦è§„åˆ’**: èƒ½å¤Ÿè¿›è¡Œå¤šæ­¥å‰ç»ï¼Œè€ƒè™‘é•¿æœŸåæœ
- **æ™ºèƒ½å‰ªæ**: åŸºäºä»·å€¼ç½‘ç»œçš„æ™ºèƒ½æœç´¢å‰ªæ
- **ä¸ç¡®å®šæ€§æ„ŸçŸ¥**: è€ƒè™‘é¢„æµ‹ä¸ç¡®å®šæ€§çš„é²æ£’å†³ç­–
- **é«˜æ•ˆæ¨¡æ‹Ÿ**: é¿å…çœŸå®ç¯å¢ƒäº¤äº’çš„é«˜æˆæœ¬

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. å¿«é€Ÿå¯åŠ¨

```bash
# å¯åŠ¨MCTSå¢å¼ºè®­ç»ƒ
./start_mcts_training.sh

# æˆ–æ‰‹åŠ¨å¯åŠ¨
python train_advanced.py --cfg config_mcts.yaml --tag mcts_training
```

### 2. é…ç½®è°ƒä¼˜

#### åŸºç¡€MCTSå‚æ•°
```yaml
mcts:
  enable_mcts: true             # å¯ç”¨MCTSè§„åˆ’
  num_simulations: 80           # æ¨¡æ‹Ÿæ¬¡æ•°
  exploration_constant: 1.2     # æ¢ç´¢å¸¸æ•°
  max_depth: 15                 # æœ€å¤§æœç´¢æ·±åº¦
  temperature: 1.0              # åŠ¨ä½œé€‰æ‹©æ¸©åº¦
```

#### é«˜çº§MCTSç‰¹æ€§
```yaml
mcts:
  enable_uncertainty_aware: true # å¯ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥
  adaptive_simulations: true    # è‡ªé€‚åº”æ¨¡æ‹Ÿæ¬¡æ•°
  progressive_widening: true    # æ¸è¿›å¼æ‰©å±•
  cache_predictions: true       # ç¼“å­˜é¢„æµ‹ç»“æœ
```

### 3. æ€§èƒ½ç›‘æ§

```bash
# å¯åŠ¨TensorBoard
tensorboard --logdir runs/

# æŸ¥çœ‹MCTSç»Ÿè®¡ä¿¡æ¯
# åœ¨TensorBoardä¸­æŸ¥çœ‹MCTS/æ ‡ç­¾ä¸‹çš„æŒ‡æ ‡
```

## ğŸ” æŠ€æœ¯ç»†èŠ‚

### 1. éšè—çŠ¶æ€å¤„ç†

MCTSæ˜¯æ— çŠ¶æ€çš„æ ‘æœç´¢ï¼Œä½†LNNæ˜¯å¸¦çŠ¶æ€çš„ã€‚æˆ‘ä»¬å·§å¦™åœ°è§£å†³äº†è¿™ä¸ªçŸ›ç›¾ï¼š

```python
# MCTSæœç´¢è·å¾—æœ€ä½³åŠ¨ä½œ
best_action = self.mcts_planner.search(initial_state=state, 
                                     initial_hidden_state=hidden_state)

# é€šè¿‡LNNæ­£å‘ä¼ æ’­ä¿æŒéšè—çŠ¶æ€è¿ç»­æ€§
_, _, new_hidden_state = self.actor(state_t, hidden_state)
```

### 2. ä¸–ç•Œæ¨¡å‹é›†æˆ

```python
def _expand(self, node, initial_hidden_state):
    # ä½¿ç”¨Actorç½‘ç»œç”Ÿæˆå€™é€‰åŠ¨ä½œ
    candidate_actions, candidate_log_probs, _ = self.actor.generate_candidate_actions(
        state_tensor, initial_hidden_state
    )
    
    # ä½¿ç”¨ä¸–ç•Œæ¨¡å‹é¢„æµ‹ä¸‹ä¸€çŠ¶æ€
    for action in candidate_actions:
        next_state, reward, done = self.world_model.sample_prediction(state_tensor, action)
        child = node.add_child(next_state, action, prior_p)
```

### 3. ä»·å€¼ç½‘ç»œæŒ‡å¯¼

```python
def _simulate(self, node, initial_hidden_state):
    # ä½¿ç”¨Criticç½‘ç»œè¯„ä¼°èŠ‚ç‚¹ä»·å€¼
    q1, q2, _, _ = self.critic(state_tensor, action_tensor, None, None)
    value = torch.min(q1, q2).item()
    return value
```

## âš™ï¸ å‚æ•°è°ƒä¼˜æŒ‡å—

### 1. æ¨¡æ‹Ÿæ¬¡æ•°è°ƒä¼˜

- **è®­ç»ƒé˜¶æ®µ**: 50-100æ¬¡æ¨¡æ‹Ÿï¼Œå¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦
- **è¯„ä¼°é˜¶æ®µ**: 100-200æ¬¡æ¨¡æ‹Ÿï¼Œè¿½æ±‚æœ€ä½³æ€§èƒ½
- **å®æ—¶åº”ç”¨**: 20-50æ¬¡æ¨¡æ‹Ÿï¼Œä¿è¯å®æ—¶æ€§

### 2. æ¢ç´¢å¸¸æ•°è°ƒä¼˜

- **é«˜æ¢ç´¢**: exploration_constant = 2.0ï¼Œé€‚åˆæ¢ç´¢é˜¶æ®µ
- **å¹³è¡¡**: exploration_constant = 1.414ï¼Œé€‚åˆå¤§å¤šæ•°æƒ…å†µ
- **ä½æ¢ç´¢**: exploration_constant = 1.0ï¼Œé€‚åˆåˆ©ç”¨é˜¶æ®µ

### 3. æœç´¢æ·±åº¦è°ƒä¼˜

- **æµ…å±‚æœç´¢**: max_depth = 10ï¼Œé€‚åˆç®€å•åœºæ™¯
- **ä¸­å±‚æœç´¢**: max_depth = 15ï¼Œé€‚åˆä¸­ç­‰å¤æ‚åº¦
- **æ·±å±‚æœç´¢**: max_depth = 20ï¼Œé€‚åˆå¤æ‚åœºæ™¯

## ğŸ”§ æ•…éšœæ’é™¤

### 1. å¸¸è§é—®é¢˜

#### è®­ç»ƒé€Ÿåº¦æ…¢
```yaml
# è§£å†³æ–¹æ¡ˆï¼šå‡å°‘æ¨¡æ‹Ÿæ¬¡æ•°
mcts:
  num_simulations: 50  # ä»100å‡å°‘åˆ°50
  max_depth: 10        # ä»20å‡å°‘åˆ°10
```

#### å†…å­˜ä¸è¶³
```yaml
# è§£å†³æ–¹æ¡ˆï¼šä¼˜åŒ–é…ç½®
mcts:
  cache_predictions: false  # å…³é—­é¢„æµ‹ç¼“å­˜
  parallel_simulations: false  # å…³é—­å¹¶è¡Œæ¨¡æ‹Ÿ
```

#### å†³ç­–è´¨é‡ä¸ä½³
```yaml
# è§£å†³æ–¹æ¡ˆï¼šå¢åŠ æ¨¡æ‹Ÿæ¬¡æ•°å’Œæ·±åº¦
mcts:
  num_simulations: 150  # å¢åŠ æ¨¡æ‹Ÿæ¬¡æ•°
  max_depth: 20         # å¢åŠ æœç´¢æ·±åº¦
  exploration_constant: 1.2  # è°ƒæ•´æ¢ç´¢å¸¸æ•°
```

### 2. è°ƒè¯•æŠ€å·§

#### æŸ¥çœ‹MCTSç»Ÿè®¡ä¿¡æ¯
```python
# è·å–MCTSç»Ÿè®¡ä¿¡æ¯
mcts_stats = agent.get_mcts_statistics()
print(f"å¹³å‡æ¨¡æ‹Ÿæ·±åº¦: {mcts_stats['average_depth']}")
print(f"ä»·å€¼ä¼°è®¡èŒƒå›´: {mcts_stats['min_value_estimate']} - {mcts_stats['max_value_estimate']}")
```

#### å¯è§†åŒ–æœç´¢æ ‘
```python
# è·å–å†³ç­–ä¿¡æ¯
decision_info = agent.get_decision_info(state)
if decision_info['decision_mode'] == 'mcts_planning':
    print(f"MCTSé…ç½®: {decision_info['mcts_config']}")
    print(f"MCTSç»Ÿè®¡: {decision_info['mcts_stats']}")
```

## ğŸ“ˆ è¿›é˜¶ä¼˜åŒ–

### 1. è‡ªé€‚åº”MCTS

```python
class AdaptiveMCTS_Planner(MCTS_Planner):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.performance_history = deque(maxlen=100)
    
    def adaptive_simulations(self, current_performance):
        # æ ¹æ®æ€§èƒ½åŠ¨æ€è°ƒæ•´æ¨¡æ‹Ÿæ¬¡æ•°
        if current_performance > 0.9:
            return min(self.num_simulations, 50)  # é«˜æ€§èƒ½æ—¶å‡å°‘æ¨¡æ‹Ÿ
        else:
            return max(self.num_simulations, 100)  # ä½æ€§èƒ½æ—¶å¢åŠ æ¨¡æ‹Ÿ
```

### 2. å¹¶è¡ŒMCTS

```python
import multiprocessing as mp

class ParallelMCTS_Planner(MCTS_Planner):
    def __init__(self, *args, num_processes=4, **kwargs):
        super().__init__(*args, **kwargs)
        self.num_processes = num_processes
    
    def parallel_search(self, initial_state):
        # å¹¶è¡Œæ‰§è¡Œå¤šä¸ªMCTSæœç´¢
        with mp.Pool(self.num_processes) as pool:
            results = pool.map(self._single_search, 
                             [initial_state] * self.num_processes)
        return self._combine_results(results)
```

### 3. å±‚æ¬¡åŒ–MCTS

```python
class HierarchicalMCTS_Planner(MCTS_Planner):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.high_level_planner = MCTS_Planner(*args, **kwargs)
        self.low_level_planner = MCTS_Planner(*args, **kwargs)
    
    def hierarchical_search(self, initial_state):
        # é«˜å±‚è§„åˆ’ï¼šç²—ç²’åº¦å†³ç­–
        high_level_action = self.high_level_planner.search(initial_state)
        
        # ä½å±‚è§„åˆ’ï¼šç»†ç²’åº¦æ‰§è¡Œ
        low_level_action = self.low_level_planner.search(initial_state)
        
        return self._combine_actions(high_level_action, low_level_action)
```

## ğŸ¯ é¢„æœŸæ•ˆæœ

é€šè¿‡é›†æˆMCTSï¼Œæ‚¨çš„æ™ºèƒ½ä½“å°†å®ç°ï¼š

1. **æ·±åº¦æ€è€ƒèƒ½åŠ›**: èƒ½å¤Ÿè¿›è¡Œå¤šæ­¥å‰ç»ï¼Œè€ƒè™‘é•¿æœŸåæœ
2. **æ™ºèƒ½å†³ç­–**: åœ¨å¤æ‚åœºæ™¯ä¸­åšå‡ºæ›´ä¼˜çš„å†³ç­–
3. **é²æ£’æ€§æå‡**: å¯¹ä¸ç¡®å®šæ€§å’Œå™ªå£°çš„é€‚åº”èƒ½åŠ›å¢å¼º
4. **æ³›åŒ–èƒ½åŠ›**: åœ¨æ–°ç¯å¢ƒä¸­çš„è¡¨ç°æ›´åŠ ç¨³å®š

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **MCTSç®—æ³•**: Browne, C., et al. "A survey of Monte Carlo tree search methods."
2. **AlphaGo**: Silver, D., et al. "Mastering the game of Go with deep neural networks and tree search."
3. **å¼ºåŒ–å­¦ä¹ ä¸è§„åˆ’**: Sutton, R. S., & Barto, A. G. "Reinforcement learning: An introduction."

---

**æ€»ç»“**: MCTSé›†æˆå°†æ‚¨çš„æ™ºèƒ½ä½“ä»"ååº”å¼"æå‡ä¸º"æ·±æ€å¼"ï¼Œå®ç°äº†è§„åˆ’ä¸å­¦ä¹ çš„å®Œç¾èåˆã€‚è¿™å°†æ˜¯ä¸€ä¸ªçœŸæ­£å…·å¤‡æ·±åº¦æ€è€ƒèƒ½åŠ›çš„æ™ºèƒ½ç³»ç»Ÿï¼
